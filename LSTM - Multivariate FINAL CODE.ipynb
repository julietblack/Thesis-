{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the same as the LSTM-multivariate Final Copy. \n",
    "The codes are just put in the same cell for easier run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.python import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, Adam\n",
    "import tensorflow as tf\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(374, 4)\n",
      "(374,)\n",
      "(94, 4)\n",
      "(94,)\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\geopy\\france_cleaned\"\n",
    "#load data\n",
    "region = \"Alsace.csv\"\n",
    "region_name = region.split('.')[0]\n",
    "df = pd.read_csv(path+'\\\\'+region)\n",
    "\n",
    "#feature engineer\n",
    "def make_cyclic_sin(month):\n",
    "    a = np.sin((month-1)*(2.*np.pi/12))\n",
    "    return a\n",
    "def make_cyclic_cos(month):\n",
    "    b = np.cos((month-1)*(2.*np.pi/12))\n",
    "    return b \n",
    "\n",
    "# filter the necessary values for temp. prediction \n",
    "temp_df = df[df['PARAMETER'] == 'T2M']\n",
    "temp_df = temp_df.drop('ANN', axis = 1)\n",
    "\n",
    "# data preprocessing \n",
    "temp_df = temp_df.rename(columns = {'YEAR':'year','JAN':1, 'FEB':2,'MAR':3, 'APR':4, 'MAY':5,'JUN':6,\n",
    "                         'JUL':7, 'AUG':8, 'SEP':9, 'OCT':10, 'NOV':11, 'DEC':12})\n",
    "temp_df = temp_df.drop(['PARAMETER','LAT','LON'], axis = 1)\n",
    "\n",
    "temp_df.set_index('year', inplace = True)\n",
    "df = pd.concat([temp_df.stack()], axis=1)\n",
    "df.reset_index(inplace = True)\n",
    "\n",
    "df.rename(columns = {'level_1':'month',0:'avg_temp'}, inplace = True)\n",
    "\n",
    "# encoding for months \n",
    "#cyclic encoding from deep learning notebook\n",
    "df['month_sin'] = df['month'].apply(lambda row: make_cyclic_sin(row))\n",
    "df['month_cos'] = df['month'].apply(lambda row: make_cyclic_cos(row))\n",
    "\n",
    "df['date'] = df[['year','month']].astype(str).apply('-'.join, axis=1)\n",
    "\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df.set_index('date')\n",
    "\n",
    "# add the unlablled data (this is for the test set)\n",
    "unlabelled_dataset = pd.read_excel('to_predict_mult.xlsx')\n",
    "unlabelled_dataset['date'] = unlabelled_dataset[['year','month']].astype(str).apply('-'.join, axis=1)\n",
    "unlabelled_dataset.date = pd.to_datetime(unlabelled_dataset.date)\n",
    "unlabelled_dataset = unlabelled_dataset.set_index('date')\n",
    "\n",
    "# combine all the data and divide\n",
    "df = df[['year','avg_temp','month_sin','month_cos']]\n",
    "unlabelled_dataset = unlabelled_dataset[['year','avg_temp','month_sin','month_cos']]\n",
    "# fit_transform all the data including the train, valid, and test \n",
    "combined_data = pd.concat([df,unlabelled_dataset])\n",
    "\n",
    "# shifting the avg_temp to t+1 (not doing it this time)\n",
    "#combined_data['avg_temp'] = combined_data['avg_temp'].shift(1)\n",
    "\n",
    "# removing the nan value and transform all the data including the train, valid, and test\n",
    "scaler = MinMaxScaler()\n",
    "combined_data = combined_data[:] # shifted to 1 (not shifting it this time)\n",
    "combined_data[['year','avg_temp','month_sin','month_cos']] = scaler.fit_transform(combined_data)\n",
    "\n",
    "# all the labelled data from 1981 to 2019\n",
    "data = combined_data[0:468] # 1981-01 -> 2019-12\n",
    "# Train-valid split: 80 percent for the training set and 20 percent for the validation on the labelled data\n",
    "y = data.avg_temp\n",
    "X = data\n",
    "valid_and_test_size = 0.1\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=valid_and_test_size*2, random_state=42, shuffle=False)\n",
    "\n",
    "#print the shape of the split data. train = (374,4),valid = (94,4)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# from series convert to array\n",
    "y_train = y_train.values.reshape(len(y_train),1)\n",
    "#y_test = y_test.values.reshape(len(y_test),1)\n",
    "y_valid = y_valid.values.reshape(len(y_valid),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>avg_temp</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126350</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-02-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.173813</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-03-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.434697</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-04-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.508347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-05-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.621931</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-06-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.720458</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-07-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.762357</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-08-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781015</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-09-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.693290</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-10-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.487398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-11-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.325041</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-12-01</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.184288</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-01-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.194108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-02-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.245172</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-03-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.336170</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-04-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.429460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-05-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.632406</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-06-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.760065</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-07-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.841571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-08-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.767594</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-09-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.745008</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-10-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.510311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-11-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.391162</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982-12-01</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.257283</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-01-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.272668</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-02-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.143044</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-03-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.353191</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-04-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.485434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-05-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.558756</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983-06-01</th>\n",
       "      <td>0.050</td>\n",
       "      <td>0.762029</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-09-01</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.712602</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-10-01</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.508020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-11-01</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.433715</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-12-01</th>\n",
       "      <td>0.700</td>\n",
       "      <td>0.227496</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.111620</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.201309</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.333879</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.508020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.574795</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.770540</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.886743</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-08-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.773486</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-09-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.637971</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-10-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.485106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.352209</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-01</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.109984</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.223241</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.263830</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-03-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.394435</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-04-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.602291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-05-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.701146</td>\n",
       "      <td>0.933013</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-06-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.744026</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.827823</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.747300</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-10-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.532242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-11-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.391489</td>\n",
       "      <td>0.066987</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-12-01</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.316530</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.270704</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-01</th>\n",
       "      <td>0.775</td>\n",
       "      <td>0.103110</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.933013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             year  avg_temp  month_sin  month_cos\n",
       "date                                             \n",
       "1981-01-01  0.000  0.126350   0.500000   1.000000\n",
       "1981-02-01  0.000  0.173813   0.750000   0.933013\n",
       "1981-03-01  0.000  0.434697   0.933013   0.750000\n",
       "1981-04-01  0.000  0.508347   1.000000   0.500000\n",
       "1981-05-01  0.000  0.621931   0.933013   0.250000\n",
       "...           ...       ...        ...        ...\n",
       "2011-10-01  0.750  0.532242   0.000000   0.500000\n",
       "2011-11-01  0.750  0.391489   0.066987   0.750000\n",
       "2011-12-01  0.750  0.316530   0.250000   0.933013\n",
       "2012-01-01  0.775  0.270704   0.500000   1.000000\n",
       "2012-02-01  0.775  0.103110   0.750000   0.933013\n",
       "\n",
       "[374 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back=12 # use the last 12 months (01-2019 -> 12-2019)\n",
    "FORECAST_DISTANCE=1 # forecast on the next month\n",
    "n_features = len(X_train.columns)\n",
    "\n",
    "\n",
    "from seglearn.transform import FeatureRep, SegmentXYForecast, last\n",
    "\n",
    "segmenter = SegmentXYForecast(width=look_back, step=1, y_func=last, forecast=FORECAST_DISTANCE) \n",
    "# the segmenter will segment the data according to the parameters set.\n",
    "X_train_rolled, y_train_rolled,_=segmenter.fit_transform([X_train.values],[y_train.flatten()])\n",
    "X_valid_rolled, y_valid_rolled,_=segmenter.fit_transform([X_valid.values],[y_valid.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362, 12, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "EPOCHS = 30\n",
    "DROPOUT_RATE= 0.01\n",
    "LSTM_UNIT_SIZE = 5\n",
    "\n",
    "# Build the model \n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_UNIT_SIZE, activation = 'relu',input_shape = (look_back, n_features)))\n",
    "model.add(Dropout(DROPOUT_RATE))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "#compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "# fit the model\n",
    "\n",
    "model_path = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France models\\\\\"\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n",
    "mc = ModelCheckpoint(model_path+f'{region_name}.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "                    \n",
    "history = model.fit(X_train_rolled, y_train_rolled, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = (X_valid_rolled,y_valid_rolled), callbacks = [es,mc]) #, callbacks = es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France evaluation\\\\\"\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.savefig(eval_path+f'{region_name} evaluation.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the saved model \n",
    "saved_model = load_model(model_path+f'{region_name}.h5')\n",
    "print('Model name: ',region_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the training and getting the inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combined_data[12:374])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = saved_model.predict(X_train_rolled) \n",
    "train_y_hat = pd.DataFrame(train_y_hat)\n",
    "train_y_hat = train_y_hat.rename(columns = {0:'y_hat'})\n",
    "for_inverse_scaling = combined_data[12:374] # 1882-01-01 ->2011-12-01\n",
    "for_inverse_scaling['y_hat_train'] = train_y_hat['y_hat'].values\n",
    "for_inverse_scaling_x = for_inverse_scaling[['year','y_hat_train','month_sin','month_cos']]\n",
    "inverse_scaling_x = scaler.inverse_transform(for_inverse_scaling_x)\n",
    "train_prediction = pd.DataFrame(inverse_scaling_x).rename(columns = {0:'year',1:'y_hat_train',2:'month_sin',3:'month_cos'})\n",
    "original_train = for_inverse_scaling[['year','avg_temp','month_sin','month_cos']]\n",
    "inversed_original_train = scaler.inverse_transform(original_train)\n",
    "inversed_original_train = pd.DataFrame(inversed_original_train).rename(columns = {0:'year',1:'avg_temp',2:'month_sin',3:'month_cos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversed_original_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root mean square error for the training set (actual vs. prediction)\n",
    "print('Train Mean Absolute Error:', mean_absolute_error(inversed_original_train['avg_temp'].values,train_prediction['y_hat_train'].values))\n",
    "print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(inversed_original_train['avg_temp'].values,train_prediction['y_hat_train'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "pred_plot_train = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France prediction plot train\\\\\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "aa=[x for x in range(24)]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(aa, inversed_original_train['avg_temp'].values[:24], marker='.', label=\"actual\") \n",
    "plt.plot(aa, train_prediction['y_hat_train'].values[:24], 'r', label=\"prediction for the training\")\n",
    "# plt.tick_params(left=False, labelleft=True) #remove ticks\n",
    "plt.tight_layout()\n",
    "sns.despine(top=True)\n",
    "plt.subplots_adjust(left=0.07)\n",
    "plt.ylabel('Temp', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig(pred_plot_train+f'{region_name} train.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the validation set and getting the inverse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[374+look_back:468]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y_hat = saved_model.predict(X_valid_rolled)\n",
    "# convert the prediction to dataframe to be added to the original\n",
    "#I'm doing this because scaler is fit on a (n,4) shape and predition is only (n,1): has to be the same shape so we need to map it back\n",
    "valid_y_hat = pd.DataFrame(valid_y_hat)\n",
    "valid_y_hat = valid_y_hat.rename(columns = {0:'y_hat_valid'})\n",
    "print(len(valid_y_hat))\n",
    "for_inverse_scaling_valid = combined_data[374+look_back:468] # valid set (end of training until the end of the labelled)\n",
    "display(for_inverse_scaling_valid)\n",
    "for_inverse_scaling_valid['y_hat_valid'] = valid_y_hat['y_hat_valid'].values\n",
    "for_inverse_scaling_x_valid = for_inverse_scaling_valid[['year','y_hat_valid','month_sin','month_cos']]\n",
    "inverse_scaling_x_valid = scaler.inverse_transform(for_inverse_scaling_x_valid)\n",
    "valid_prediction = pd.DataFrame(inverse_scaling_x_valid).rename(columns = {0:'year',1:'y_hat_valid',2:'month_sin',3:'month_cos'})\n",
    "# get the original valid data\n",
    "original_valid = for_inverse_scaling_valid[['year','avg_temp','month_sin','month_cos']]\n",
    "# inverse the original valid set\n",
    "inversed_original_valid = scaler.inverse_transform(original_valid)\n",
    "# put the original valid in dataframe \n",
    "inversed_original_valid = pd.DataFrame(inversed_original_valid).rename(columns = {0:'year',1:'avg_temp',2:'month_sin',3:'month_cos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('valid Mean Absolute Error:', mean_absolute_error(inversed_original_valid['avg_temp'].values,valid_prediction['y_hat_valid'].values))\n",
    "print('valid Root Mean Squared Error:',np.sqrt(mean_squared_error(inversed_original_valid['avg_temp'].values,valid_prediction['y_hat_valid'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid\n",
    "\n",
    "pred_plot_valid = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France prediction plot valid\\\\\"\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "aa=[x for x in range(24)]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(aa, inversed_original_valid['avg_temp'].values[:24], marker='.', label=\"actual\") \n",
    "plt.plot(aa, valid_prediction['y_hat_valid'].values[:24], 'r', label=\"prediction for the valid\")\n",
    "# plt.tick_params(left=False, labelleft=True) #remove ticks\n",
    "plt.tight_layout()\n",
    "sns.despine(top=True)\n",
    "plt.subplots_adjust(left=0.07)\n",
    "plt.ylabel('Temp', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.savefig(pred_plot_valid+f'{region_name} valid.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals for training\n",
    "\n",
    "#inversed_original_train['avg_temp'].values,train_prediction['y_hat_train']\n",
    "original_y_train =  inversed_original_train['avg_temp'] #y\n",
    "prediction_train = train_prediction['y_hat_train']\n",
    "df_residual_train = pd.concat([original_y_train,prediction_train], axis = 1)\n",
    "df_residual_train['residual'] = df_residual_train['avg_temp'] - df_residual_train['y_hat_train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals for training\n",
    "resid_train = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France residual plots train\\\\\"\n",
    "sns_plot = sns.residplot(x ='y_hat_train', y = 'avg_temp', data = df_residual_train, color = 'blue')\n",
    "sns_plot.figure.savefig(resid_train+f'{region_name} train residual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals for validation \n",
    "\n",
    "#inversed_original_valid['avg_temp'].values,valid_prediction['y_hat_valid'].values\n",
    "original_y_valid = inversed_original_valid['avg_temp']\n",
    "prediction_valid = valid_prediction['y_hat_valid']\n",
    "df_residual_valid = pd.concat([original_y_valid,prediction_valid], axis = 1)\n",
    "df_residual_valid['residual'] = df_residual_valid['avg_temp'] - df_residual_valid['y_hat_valid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_valid = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France residual plots valid\\\\\"\n",
    "\n",
    "sns_plot_valid = sns.residplot(x ='y_hat_valid', y = 'avg_temp', data = df_residual_valid, color = 'green')\n",
    "sns_plot_valid.figure.savefig(resid_valid+f'{region_name} valid residual.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the future dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = combined_data[456:] \n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop over to \n",
    "\n",
    "pred_list = []\n",
    "forecast_months = 24 # 2020-2021\n",
    "\n",
    "prediction = 0\n",
    "\n",
    "for row in range(forecast_months):\n",
    "    #print(row)\n",
    "    look_back = 12\n",
    "    look_back_set = test_set[row:look_back+row] # [0:12]\n",
    "    if row != 0:\n",
    "        look_back_set.iloc[11][1] = prediction\n",
    "    #print(counter)\n",
    "    #display(look_back_set)\n",
    "    x_test_reshaped = look_back_set.values.reshape(1,12,4)\n",
    "    predict = saved_model.predict(x_test_reshaped)\n",
    "    prediction = predict[0][0] ## append this to the next test set\n",
    "    pred_list.append(prediction) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_prediction = pd.DataFrame(pred_list).rename(columns = {0:'prediction'})\n",
    "test_set[12:]['avg_temp'] =  test_set_prediction['prediction'].values\n",
    "inverse_test_prediction = test_set[12:][['year','avg_temp','month_sin','month_cos']]\n",
    "inverse_test_prediction = scaler.inverse_transform(inverse_test_prediction)\n",
    "final_prediction = pd.DataFrame(inverse_test_prediction).rename(columns = {0:'year',1:'predicted_temp',2:'month_sin',3:'month_cos'})\n",
    "temp_prediction_path = r\"C:\\Users\\rezan\\Documents\\Master in Applied Data Science\\2nd Year\\MASTER THESIS\\GIT THESIS\\Multivariate LSTM\\France Predictions\\\\\"\n",
    "display(final_prediction)\n",
    "final_prediction.to_excel(temp_prediction_path+f' {region_name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
